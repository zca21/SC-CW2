---
title: "Untitled"
output: html_document
date: "2022-12-12"
---
```{r}
setwd("~/Desktop/Statistical Computing/CW/SC-CW2")
```


```{r}
#Extra code to check was doing inverse transform sampling correctly

sample.dexp <- function(n,a,b,s,lamda){
  #Sampling from the double exp function g(x)
  #Using inverse transformation technique
  X <- NULL
  for (i in 1:n) {
      u <- runif(1) 
      if (u<0.5){
        X[i] <- b+log(2*u)/lamda
      }else{
        X[i] <- b-log(2-2*u)/lamda
      }
  }
  
  return(X)
}
dexp.sample <- sample.dexp(10000,2,3,1.25,0.5)
plot(density(dexp.sample))
#Plot looks good (double exp function with mean centered at 3)
#Note that its center (mean) is b and variance or spread is controlled by lambda (b on wiki note lambda=1/b) thus for smaller lambda variance increases
```
```{r}
## Question 1

#c
q1.ar <- function(n,a,b,s,lambda,M){
  
    X <- NULL
    for (i in 1:n){
      repeat{
        #Drawing x from the propsal distibution 
        u <- runif(1)
        if (u<0.5){
          x <- b+log(2*u)/lambda
        }else {
          x <- b-log(2-2*u)/lambda
        }
        #Performing A-R sampling
        y <- runif(1)
        if (s < x){
          if(y <= (a*s^a)/(2*x^(a+1))/(M*lambda*exp(-lambda*abs(x-b))/2)){
            X[i]<-x
            break
          }
        } else if(s-b<x){
          if(y <= 1/(2*b)/(M*lambda*exp(-lambda*abs(x-b))/2)){
            X[i]<-x
            break
          }
        }
      }
    }
    return(X)
}


#d
set.seed(2022)
ar.sample <- q1.ar(5000,2,3,1.25,0.5,M=3.2*exp(0.875))
#Creating function to create actual pdf of target distribution
target.dist <- function(x,a,b,s){
  n<-length(x)
  y<-rep(0,n)
  for (i in 1:n){
  
  if (x[i]<s-b){
    y[i] <- 0
  }else if (x[i]>s){
    y[i] <- (a*s^a)/(2*x[i]^(a+1))
  } else {
    y[i] <- 1/(2*b)
  }
  }
  return(y)
}

#Creating curve of target distribution
hist(ar.sample,nclass=100,prob = T,
     xlab = "Values generated by AR sampling",
     main = "Simulated and actual pdf of target distribution");
curve(target.dist(x,2,3,1.25),from=-4,to=20,n=1000,add=T, lwd= 1.5, lty= 3,col="red");
legend("topright", lty = c(1, 3), cex = 0.6, col = c("black","red"),
legend = c("Simulated pdf", "Actual pdf"))


#e
#Using the plain Monte Carlo estimator of I (I_{n})

#Function to calculate the kth root of kth moment using plain Monte Carlo estimator
k.root.k.moment <- function(sample,n,k){
  output <- (1/n*sum(sample^k))^(1/k)
  return(output)
}
  
#k=2
round(k.root.k.moment(ar.sample,5000,2),3)
#2nd root of 2nd moment is 2.204 (3 d.p.)

#k=3
round(k.root.k.moment(ar.sample,5000,3),3)
#3rd root of 3rd moment is 3.350 (3 d.p.)
```

```{r}
## Question 2

# Trial function calculates probability of X_1 >a_1 given initial value (need to now work out how to do the rest)
q2.is.trial <- function(n,x0,time,a,lambda){
  #Turning values into proportions
  a <- a/100
  x0 <- x0/100
  #transforming a to logit scale and turning a into a proportion (dividing by 100)
  b <- log((a)/(1-a))
  #generating samples from shifted exponential
  X <- rexp(n,rate = lambda) + (b-x0)/sqrt(time[2]-time[1])
  #Calculating weights
  v.weights <- dnorm(X)/dexp(X-((b-x0)/sqrt(time[2]-time[1])),rate=lambda)
  #Finding I.S estimator
  p.t <- mean(v.weights)
  return(p.t)
}

q2.is.trial(n=10000,x0=30,time=c(0,0.7),a=50,lambda=0.5)


q2.is.trial.chain <- function(n,x0,time,a,lambda){
  #Turning values into proportions
  a <- a/100
  x0 <- x0/100
  #transforming a to logit scale (calling this vector b)
  b <- log((a)/(1-a))
  
  p.b.iter <- NULL
  
  if (length(time)>2){  #Case when Markov chain has more than 1 transition thus have to simulate a random walk to calculate the proposal distribution 
  
    for (k in 1:n){

      #Simulating random walk
      r.w <- NULL
      r.w[1] <- x0
      for (i in 2:(length(time)-1)){
        r.w[i] <- rnorm(1,mean=r.w[i-1],sd=sqrt(time[i]-time[i-1]))
      }
      
      #generating samples from shifted exponential
      X <- NULL
      for (i in 2:length(time)){
        X[i-1] <- rexp(1,rate = lambda) + (b[i-1]-r.w[i-1])/sqrt(time[i]-time[i-1])
      }
      #Calculating weights for each observation in random walk
      v.weights <- NULL
      for (i in 2:length(time)){
        v.weights[i-1] <- dnorm(X[i-1])/dexp(X[i-1]-((b[i-1]-r.w[i-1])/sqrt(time[i]-time[i-1])),rate=lambda)
      }
      #Finding probability that X is greater than a (satisfies equation 6) for current iteration 
      p.b.iter[k] <- prod(v.weights)
      }
      #Finding I.S estimator by taking mean 
      I.S.est <- mean(p.b.iter)
  
  } else { # Dealing with case that markov chain only has 1 jump so don't need to calculate random walk
           # Simplifies to case that X_{1}>a given that X_{0}=x0
    
    #generating samples from proposal shifted exponential
    X <- rexp(n,rate = lambda) + (b-x0)/sqrt(time[2]-time[1])
    #Calculating weights
    v.weights <- dnorm(X)/dexp(X-((b-x0)/sqrt(time[2]-time[1])),rate=lambda)
    #Finding I.S estimator
    I.S.est  <- mean(v.weights)
  }
  
  return(I.S.est)
}
set.seed(2022)
q2.is.trial.chain(n=10000,x0=30,time=c(0,0.7,1.2,2.5),a=c(50,70,90),lambda=0.5)
set.seed(2022)
q2.is.trial.chain(n=10000,x0=30,time=c(0,0.7),a=c(50),lambda=0.5)


set.seed(2022)
n=10000
x0=5
time=c(0,0.7,1.2,2.5)
a=c(50,70,90)
lambda=0.5


  a <- a/100
  x0 <- x0/100
  #transforming a to logit scale and turning a into a proportion (dividing by 100)
  b <- log((a)/(1-a))
  p.t <- NULL
  for (k in 1:n){

  #Simulating random walk
  r.w <- NULL
  r.w[1] <- x0
  for (i in 2:(length(time)-1)){
    r.w[i] <- rnorm(1,mean=r.w[i-1],sd=sqrt(time[i]-time[i-1]))
  }
  
  #generating samples from shifted exponential
  X <- NULL
  for (i in 2:length(time)){
  X[i-1] <- rexp(1,rate = lambda) + (b[i-1]-r.w[i-1])/sqrt(time[i]-time[i-1])
  }
  #Calculating weights for each observation in random walk
  v.weights <- NULL
  for (i in 2:length(time)){
  v.weights[i-1] <- dnorm(X[i-1])/dexp(X[i-1]-((b[i-1]-r.w[i-1])/sqrt(time[i]-time[i-1])),rate=lambda)
  }
  #Finding I.S estimator (product means to find overall)
  p.t[k] <- prod(mean(v.weights))
  }
  




n <- 100
x0 <- 30
time <- c(0,0.7)
a <- 50
lambda <- 0.5

  b <- log((a/100)/(1-(a/100)))
  #generating samples from shifted exponential
  X <- rexp(n,rate = lambda) + (b-x0)/sqrt(time[2]-time[1])
  v.weights <- dnorm(X)/dexp(X-((b-x0)/sqrt(time[2]-time[1])),rate=lambda)
  p.t <- mean(v.weights)
  return(p.t)
```

```{r}
## Question 3

#start by doing b and c 
#then do d awhile figuring out a (the algorithm part)

#Part b
calcDirichletPDF <- function(x,a){
  #Calculating density of x under Dirichlet distribution with parameters specified by a
  density <- (gamma(sum(a))/prod(gamma(a)))*prod(x^(a-1))
  #calculating log of density
  log.density <- log(density)
  #returning a numeric vector
  return(c(log.density))
}

#Part c
simDirichletRV <- function(a){
  #Drawing from gamma distribution
  U <- rgamma(a,1)
  #Computing V a vector drawn from Dirichlet distribution with parameters specified by a
  V <- U/sum(U)
  
  return(V)
}

#Part d
q3.mcmc <- function(X,initStateProbs,initPT,burnin,n){
  #X is vector of weather for each observed day (d long if recorded weather for d days)
  #Converting X into a numeric vector to access entries of PT
  X <- ifelse(X=="s",1,(ifelse(X=="c",2,3)))
  init.state <- X[1:(length(X)-1)]
  trans.state <- X[2:length(X)]
  
  #initializing algorithm
  V <- array(NA,dim=c(3,3,n+burnin+1))
  V[,,1] <- initStateProbs
  
  for (j in 2:(n+burnin+1)){
    proposal <- matrix(NA,ncol=3,nrow=3)
    #Generating proposal value for each row of transition matrix
    for (i in 1:length(initPT)){
      proposal[i,] <- simDirichletRV(20*V[,i,j-1])
    }
      
      #Generating posterior using equation (12) for proposal and old value of PT
      prob.trans.prev <- NULL
      prob.trans.curr <- NULL
      for (i in 1:length(init.state)){
        prob.trans.prev[i] <- V[init.state[i],trans.state[i],j-1]
        prob.trans.curr[i] <- proposal[trans.state[i],init.state[i]]
      }
      #density has some constant as proportional but due to fraction constant cancels out so dont
      #need to worry about it
      density.prev <- prod(prob.trans.prev)*initStateProbs[X[1]]
      density.curr <-prod(prob.trans.curr)*initStateProbs[X[1]]
      
      #alpha simplier as q is symmetric (Big boi assumption may not be able to do this)
      alpha <- min(1,density.curr/density.prev)
      
      #Accept or Reject
      if (runif(1) <= alpha){
        V[,,j] <- proposal
      } else{
        V[,,j] <- V[,,j-1]
      }
  }
  
  return(V[,,(burnin+1:n+burnin+1)])
  
}
#finding prob of sequence under current pt
X <- read.table("q3Weather.txt",header = F)
X <- X$V1


#Part e
initPT <- matrix(1/3,nrow=3,ncol=3)
initStateProbs <- c(1/3,1/3,1/3)

q3.mcmc(X,initStateProbs,initPT,10000,100000)
```

```{r}
  #initializing algorithm
  V <- array(NA,dim=c(3,3,500))
  V[,,1] <- initStateProbs
  
  V
  
  for (j in 2:(n+burnin+1)){
    proposal <- matrix(NA,ncol=3,nrow=3)
    #Generating proposal value for each row of transition matrix
    for (i in 1:length(initPT)){
      x <- V[,i,j-1]
      proposal[1,] <- simDirichletRV(20*V[,1,2-1])
    }
   test <- matrix(c(1,2,3,4),ncol=2,nrow=2) 
test[2,1]
```



```{r}
## Question 4
q4.data <- read.csv("q4MixLinReg.csv",header=T)

#Part a
linRegMixEstep <- function(piCur,aCur,bCur,sCur,x,Y){
  #matrix to store E-Step values
  #row i (p(Z_{i} = 1|Y_{i}, θ^{cur}),..., p(Z_{i} = K|Y_{i}, θ^{cur})) row i has X_{i} ie X fixed
  #column j (p(Z_{i} = j|Y_{1}, θ^{cur}), ...,(Z_{i} = k|Y_{k}, θ^{cur})) column j has Z_{i}=j  ie value of Z_{i} fixed
  prob <- matrix(data=NA,nrow = length(x),ncol = length(piCur))
  for (i in 1:length(x)){
    for (j in 1:length(piCur)){
      prob[i,j] <- piCur[j]*dnorm(Y[i],mean=aCur[j]+bCur[j]*x[i],sd=sCur[j])/sum(piCur*dnorm(Y[i],mean=aCur+bCur*x[i],sd=sCur))
    }
  }
  return(prob)
}


#Part b
#i
calcNewCoefs <- function(W,x,Y){
  #Calculating a^{new) and b^{new}
  new.coef <- matrix(data=NA,ncol = 2,nrow=dim(W)[2])
  #Creating design matrix
  X <- matrix(c(rep(1,length(x)),x),ncol=2)
  for (k in 1:dim(W)[2]){
    new.coef[k,] <- solve(t(X)%*%diag(W[,k])%*%X)%*%t(X)%*%diag(W[,k])%*%Y
  }
  return(t(new.coef))
}

#ii
calcNewSd <- function(W,x,Y,a,b){
  new.sd <- NULL
  #calculating new sd
  for (k in 1:dim(W)[2]){
    new.sd[k] <- sqrt(sum(W[,k]*(Y-a[k]-b[k]*x)^2)/sum(W[,k]))
  }
  return(new.sd)
}

#iii
linRegMixMstep <- function(W,x,Y){
  #calculating new coefficents for a and b
  new.coef <- calcNewCoefs(W,x,Y)
  
  #calculating new sd
  new.sd <- calcNewSd(W,x,Y,new.coef[1,],new.coef[2,]) 

  #Calculating new pi
  new.pi <- NULL
  for (k in 1:dim(W)[2]){
    new.pi[k] <- sum(W[,k])/dim(W)[1]
  }
 return(list(new.pi,new.coef[1,],new.coef[2,],new.sd))
}


#Part c
linRegMixCalcLogLik <- function(x,Y,piCur,piNew,aCur,aNew,bCur,bNew,sCur,sNew){
  #calculating conditional probabilities for current and new values of theta
  W.cur <- linRegMixEstep(piCur,aCur,bCur,sCur,x,Y)
  W.new <- linRegMixEstep(piNew,aNew,bNew,sNew,x,Y)
  #Creating object to store values of log-likelihood for each observation i of Y 
  Q.cur <- NULL
  Q.new <- NULL
  for (i in 1:length(Y)){
    #calculating log likelihood for each value of Y
    Q.cur[i] <- sum(W.cur[i,]*log(piCur))+sum(W.cur[i,]*log(dnorm(Y[i],mean = aCur+bCur*x[i],sd=sCur)))
    Q.new[i] <- sum(W.new[i,]*log(piNew))+sum(W.new[i,]*log(dnorm(Y[i],mean = aNew+bNew*x[i],sd=sNew)))
  }
  #Summing over all observations as is in the formula
  Q.cur <- sum(Q.cur)
  Q.new <- sum(Q.new)
  return(c(Q.cur,Q.new))
}


#Part d
linRegMixEM <- function(piInit,aInit,bInit,sInit,x,Y,convergeEps){
  #Initializing the algorithm
  piCur <- piInit
  aCur <- aInit
  bCur <- bInit
  sCur <- sInit
  
  conv.critera <- 0
  while(conv.critera==0){
    #E-step
    W <- linRegMixEstep(piCur,aCur,bCur,sCur,x,Y)
    #Calculating new values for parameters
    theta.new <- linRegMixMstep(W,x,Y)
    #Calculating log-likelihood for current and new values of parameters
    theta.loglik <- linRegMixCalcLogLik(x,Y,piCur,theta.new[[1]],aCur,theta.new[[2]],bCur,theta.new[[3]],sCur,theta.new[[4]])
    #checking convergence criteria
    if(abs(theta.loglik[2]-theta.loglik[1])<convergeEps){
      conv.critera=1
    }
    #Updating current values of parameters
    piCur <- theta.new[[1]]
    aCur <- theta.new[[2]]
    bCur <- theta.new[[3]]
    sCur <- theta.new[[4]]
  }
  return(theta.new)
}

#Part e
mle.est <- linRegMixEM(c(0.4,0.3,0.3),c(0.1,-0.1,0.2),c(1,-1,1),c(1,0.5,1.1),q4.data$x,q4.data$Y,1e-06)
#pi MLEs
round(mle.est[[1]],3)
# (0.450, 0.255, 0.295)

#alpha MLEs
round(mle.est[[2]],3)
# (0.838,  7.458, -5.308)

#beta MLEs
round(mle.est[[3]],3)
# (0.258, -1.755,  1.223)

#sd MLEs
round(mle.est[[4]],3)
# (0.787, 0.935, 1.450)
```

```{r}
#Q5 but with unknown number of groups 

#Need to edit as in bootstrapping dont care aout having same number in each group! (I think)

#a
bootFStat <- function(group,y,w,bootCount){
  #object to store B bootstrap estimates of f statistic
  Fstat.boot.est <- NULL
  #grouping values into dataset 
  data <- data.frame("group"=group,"Obs"=y,"weight"=w)
  #Adding id column so can sample by id to retain all info of row (obs and weight)
  data$id <- seq(1,dim(data)[1])
  
  for (b in 1:bootCount){
    #object to store value of y tidle * N 
    y.tilde.N <- 0
    
    #loop to do bootstrap method for each group
      for (group_val in unique(data$group)){
        #selecting subset id's of dataset that is current group
        group_dataset <- data$id[data$group==group_val]
        #sampling from group dataset with replacement
        group_sample.id <- sample(group_dataset, size = length(group_dataset), replace = TRUE)
        
          #Using sampled id to select obs and weight for each id sampled to create bootstrap dataset for each group
          #Selecting obs and weights using id's sampled
          sample.group.obs <- NULL
          sample.group.weight <- NULL
          
          for (i in 1:length(group_sample.id)){
            sample.group.obs[i] <- data$Obs[data$id == group_sample.id[i]]
            sample.group.weight[i] <- data$weight[data$id == group_sample.id[i]]
          }
          #Setting sampled observations and weights to be those of group they belong to before repeating loop with next group
          assign(paste0("sample.",group_val,".obs"),sample.group.obs)
          assign(paste0("sample.",group_val,".weight"),sample.group.weight)
          
          #Calculating y tilde_{i} for current group
          y.tilde.group <- sum(sample.group.obs*sample.group.weight)/length(sample.group.obs)
          assign(paste0("y.tilde.",group_val),y.tilde.group)
          
          #updating y tilde using current group
          y.tilde.N <- y.tilde.N + sum(sample.group.obs*sample.group.weight)
          
          #Calculating part of denominator of f-statistic corresponding to current group
          f.denom.group.part <- sum((sample.group.obs*sample.group.weight-y.tilde.group)^2)
          assign(paste0("f.denom.",group_val),f.denom.group.part)
      }
    
    #calculating y tilde
    y.tilde <- y.tilde.N/length(y)
    # y.tilde <- y.tilde.N/300
    
    #calculating summation parts of numerator and denominator of f statistic
    fstat.num <- 0
    fstat.denom <- 0
    for (group_val in unique(data$group)){
      fstat.num <- fstat.num+length(get(paste0("sample.",group_val,".obs")))*(get(paste0("y.tilde.",group_val))-y.tilde)^2
      fstat.denom <- fstat.denom + get(paste0("f.denom.",group_val))
    }
    #calculating F stat
    fstat <- (fstat.num/(length(unique(group))-1))/(fstat.denom/(length(y)-length(unique(group))))
    Fstat.boot.est[b] <- fstat
  }
  
  #Calculating standard error of bootstrap f statistic
  bootstr.mu <- sum(Fstat.boot.est)/length(Fstat.boot.est)
  bootstr.se <- sqrt(sum((Fstat.boot.est-bootstr.mu )^2)/(length(Fstat.boot.est-1)))
  
  return(bootstr.se)
}

#b
set.seed(2022)
bootFStat(eco.df$habitat,eco.df$density,eco.df$weight,1000)  



```

```{r}
#Bootstrap where number in each group not set

bootFStat <- function(group,y,w,bootCount){
  #object to store B bootstrap estimates of f statistic
  Fstat.boot.est <- NULL
  #grouping values into dataset 
  data <- data.frame("group"=group,"Obs"=y,"weight"=w)
  #Adding id column so can sample by id to retain all info of row (obs and weight)
  data$id <- seq(1,dim(data)[1])
  
  for (b in 1:bootCount){
    #object to store value of y tilde * N 
    y.tilde.N <- 0
    
    #sampling dataset
    sample.id <- sample(data$id, size = length(y), replace = TRUE)

    sample.obs <- NULL
    sample.weight <- NULL
    sample.group <- NULL
    #extracting weight, group and y value corresponding to id sampled
    for (i in 1:length(y)){
      sample.obs[i] <- data$Obs[data$id == sample.id[i]]
      sample.weight[i] <- data$weight[data$id == sample.id[i]]
      sample.group[i] <- data$group[data$id == sample.id[i]]
    }
    #creating dataset of sample
    sample.dataset <- data.frame("group"=sample.group,"Obs"=sample.obs,"weight"=sample.weight)
    
    #computing statistics to calculate f-stat
    for (group_val in unique(sample.dataset$group)){
      group.dataset <- data.frame("Obs"=sample.dataset$Obs[sample.dataset==group_val],
                                  "weight"=sample.dataset$weight[sample.dataset==group_val])
      
      #calculating number of obs for current group
      assign(paste0("nobs.",group_val),dim(group.dataset)[1])
      
      #Calculating y tilde_{i} for current group
      y.tilde.group <- sum(group.dataset$Obs*group.dataset$weight)/dim(group.dataset)[1]
      assign(paste0("y.tilde.",group_val),y.tilde.group)
          
      #updating y tilde using current group
      y.tilde.N <- y.tilde.N + sum(group.dataset$Obs*group.dataset$weight)
          
      #Calculating part of denominator of f-statistic corresponding to current group
      f.denom.group.part <- sum((group.dataset$Obs*group.dataset$weight-y.tilde.group)^2)
      assign(paste0("f.denom.",group_val),f.denom.group.part)
      
    }
    #calculating y tilde
    y.tilde <- y.tilde.N/length(y)
    
    #calculating summation parts of numerator and denominator of f statistic
    fstat.num <- 0
    fstat.denom <- 0
    for (group_val in unique(data$group)){
      fstat.num <- fstat.num+get(paste0("nobs.",group_val))*(get(paste0("y.tilde.",group_val))-y.tilde)^2
      fstat.denom <- fstat.denom + get(paste0("f.denom.",group_val))
    }
    #calculating F stat
    fstat <- (fstat.num/(length(unique(group))-1))/(fstat.denom/(length(y)-length(unique(group))))
    Fstat.boot.est[b] <- fstat
    
  }
  #Calculating standard error of bootstrap f statistic
  bootstr.mu <- sum(Fstat.boot.est)/length(Fstat.boot.est)
  bootstr.se <- sqrt(sum((Fstat.boot.est-bootstr.mu )^2)/(length(Fstat.boot.est-1)))
  
  return(bootstr.se)
}
#Sorry about using so many loops :P

#b
set.seed(2022)
bootFStat(eco.df$habitat,eco.df$density,eco.df$weight,1000)  
```





```{r}
## Question 5 extra stuff (use this to check results by doing piece by piece)

#a
unique(eco.df$habitat)[3]

bootFStat <- function(group,y,w,bootCount){
  #grouping into dataset 
  data <- data.frame("group"=group,"Obs"=y,"weight"=w)
  #Adding id column so can sample by id to retain all info of row (obs and weight)
  data$id <- seq(1,dim(data)[1])
  
  #Splitting data into groups depending on group
  group.A <- data$id[data$group=="A"]
  group.B <- data$id[data$group=="B"]
  group.C <- data$id[data$group=="C"]
  #Sampling with replacement
  sample.A.id <- sample(group.A, size = length(group.A), replace = TRUE)
  sample.B.id <- sample(group.B, size = length(group.B), replace = TRUE)
  sample.C.id <- sample(group.C, size = length(group.C), replace = TRUE)
  
  #Using sampled id to select obs and weight for each id sampled to create bootstrap dataset for each group
  #Selecting obs and weights using id's sampled
  
  #For A
  sample.A.obs <- NULL
  sample.A.weight <- NULL
  for (i in 1:length(sample.A.id)){
    sample.A.obs[i] <- data$Obs[data$id == sample.A.id[i]]
    sample.A.weight[i] <- data$weight[data$id == sample.A.id[i]]
  }
  #calculating y.tilde_{i} for A
  y.tilde.A <- sum(sample.A.obs*sample.A.obs)/length(sample.A.obs)
  
  #For B
  sample.B.obs <- NULL
  sample.B.weight <- NULL
  for (i in 1:length(sample.B.id)){
    sample.B.obs[i] <- data$Obs[data$id == sample.B.id[i]]
    sample.B.weight[i] <- data$weight[data$id == sample.B.id[i]]
  }
  #calculating y.tilde_{i} for B
  y.tilde.B <- sum(sample.B.obs*sample.B.obs)/length(sample.B.obs)
  
  #For C
  sample.C.obs <- NULL
  sample.C.weight <- NULL
  for (i in 1:length(sample.C.id)){
    sample.C.obs[i] <- data$Obs[data$id == sample.C.id[i]]
    sample.C.weight[i] <- data$weight[data$id == sample.C.id[i]]
  }
  #calculating y.tilde_{i} for C
  y.tilde.C <- sum(sample.C.obs*sample.C.obs)/length(sample.C.obs)
  
  #Calculating y.tilde
  y.tilde <- (y.tilde.A*length(sample.A.obs)+y.tilde.B*length(sample.B.obs)+y.tilde.C*length(sample.C.obs))/length(y)
  
  #Calculating F.tilde
  F.num <- 1
  F.denom <-2
}








library(dplyr)
eco.df <- read.table("q5EcoStudy.txt",header = T)
data <- eco.df%>%
  rename("group"="habitat","Obs"="density")
data$id <- seq(1,dim(data)[1])
  
  #Splitting data into groups depending on group
  group.A <- data$id[data$group=="A"]
  group.B <- data$id[data$group=="B"]
  group.C <- data$id[data$group=="C"]
  #Sampling with replacement
  sample.A.id <- sample(group.A, size = length(group.A), replace = TRUE)
  sample.B.id <- sample(group.B, size = length(group.B), replace = TRUE)
  sample.C.id <- sample(group.C, size = length(group.C), replace = TRUE)
  
  #Using sampled id to select obs and weight for each id sampled to create bootstrap dataset for each group
  #Selecting obs and weights using id's sampled
  
  #For A
  sample.A.obs <- NULL
  sample.A.weight <- NULL
  for (i in 1:length(sample.A.id)){
    sample.A.obs[i] <- data$Obs[data$id == sample.A.id[i]]
    sample.A.weight[i] <- data$weight[data$id == sample.A.id[i]]
  }
  #calculating y.tilde_{i} for A
  y.tilde.A <- sum(sample.A.obs*sample.A.obs)/length(sample.A.obs)


#b
eco.df <- read.table("q5EcoStudy.txt",header = T)

test
```


