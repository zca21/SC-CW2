---
title: "Untitled"
output: html_document
date: "2022-12-12"
---
```{r}
setwd("~/Desktop/Statistical Computing/CW/SC-CW2")
```


```{r}
#Extra code to check was doing inverse transform sampling correctly

sample.dexp <- function(n,a,b,s,lamda){
  #Sampling from the double exp function g(x)
  #Using inverse transformation technique
  X <- NULL
  for (i in 1:n) {
      u <- runif(1) 
      if (u<0.5){
        X[i] <- b+log(2*u)/lamda
      }else{
        X[i] <- b-log(2-2*u)/lamda
      }
  }
  
  return(X)
}
dexp.sample <- sample.dexp(10000,2,3,1.25,0.5)
plot(density(dexp.sample))
#Plot looks good (double exp function with mean centered at 3)
#Note that its center (mean) is b and variance or spread is controlled by lambda (b on wiki note lambda=1/b) thus for smaller lambda variance increases
```
```{r}
## Question 1

#c
q1.ar <- function(n,a,b,s,lambda,M){
  
    X <- NULL
    for (i in 1:n){
      repeat{
        #Drawing x from the propsal distibution 
        u <- runif(1)
        if (u<0.5){
          x <- b+log(2*u)/lambda
        }else {
          x <- b-log(2-2*u)/lambda
        }
        #Performing A-R sampling
        y <- runif(1)
        if (s < x){
          if(y <= (a*s^a)/(2*x^(a+1))/(M*lambda*exp(-lambda*abs(x-b))/2)){
            X[i]<-x
            break
          }
        } else if(s-b<x){
          if(y <= 1/(2*b)/(M*lambda*exp(-lambda*abs(x-b))/2)){
            X[i]<-x
            break
          }
        }
      }
    }
    return(X)
}


#d
set.seed(2022)
ar.sample <- q1.ar(5000,2,3,1.25,0.5,M=3.2*exp(0.875))
#Creating function to create actual pdf of target distribution
target.dist <- function(x,a,b,s){
  n<-length(x)
  y<-rep(0,n)
  for (i in 1:n){
  
  if (x[i]<s-b){
    y[i] <- 0
  }else if (x[i]>s){
    y[i] <- (a*s^a)/(2*x[i]^(a+1))
  } else {
    y[i] <- 1/(2*b)
  }
  }
  return(y)
}

#Creating curve of target distribution
hist(ar.sample,nclass=100,prob = T,
     xlab = "Values generated by AR sampling",
     main = "Simulated and actual pdf of target distribution");
curve(target.dist(x,2,3,1.25),from=-4,to=20,n=1000,add=T, lwd= 1.5, lty= 3,col="red");
legend("topright", lty = c(1, 3), cex = 0.6, col = c("black","red"),
legend = c("Simulated pdf", "Actual pdf"))


#e
#Using the plain Monte Carlo estimator of I (I_{n})

#Function to calculate the kth root of kth moment using plain Monte Carlo estimator
k.root.k.moment <- function(sample,n,k){
  output <- (1/n*sum(sample^k))^(1/k)
  return(output)
}
  
#k=2
round(k.root.k.moment(ar.sample,5000,2),3)
#2nd root of 2nd moment is 2.204 (3 d.p.)

#k=3
round(k.root.k.moment(ar.sample,5000,3),3)
#3rd root of 3rd moment is 3.350 (3 d.p.)
```

```{r}
## Question 2
n <- 100000
X <- rexp(n,rate=0.5)+30 ## generate samples from shifted Exp(10/7)
v.weights <- dnorm(X,mean=30,sd=0.7)/dexp(X-30,rate=0.5) ## vector of weights
p.t <- mean(v.weights) ## calculate the IS estimator

q2.is <- function(n,x0,time,a,lambda){
  
  
}
```

```{r}
## Question 3

```


```{r}
## Question 4
q4.data <- read.csv("q4MixLinReg.csv",header=T)

#Part a
linRegMixEstep <- function(piCur,aCur,bCur,sCur,x,Y){
  #matrix to store E-Step values
  #row i (p(Z_{i} = 1|Y_{i}, θ^{cur}),..., p(Z_{i} = K|Y_{i}, θ^{cur})) row i has X_{i} ie X fixed
  #column j (p(Z_{i} = j|Y_{1}, θ^{cur}), ...,(Z_{i} = k|Y_{k}, θ^{cur})) column j has Z_{i}=j  ie value of Z_{i} fixed
  prob <- matrix(data=NA,nrow = length(x),ncol = length(piCur))
  for (i in 1:length(x)){
    for (j in 1:length(piCur)){
      prob[i,j] <- piCur[j]*dnorm(Y[i],mean=aCur[j]+bCur[j]*x[i],sd=sCur[j])/sum(piCur*dnorm(Y[i],mean=aCur+bCur*x[i],sd=sCur))
    }
  }
  return(prob)
}


#Part b
#i
calcNewCoefs <- function(W,x,Y){
  #Calculating a^{new) and b^{new}
  new.coef <- matrix(data=NA,ncol = 2,nrow=dim(W)[2])
  #Creating design matrix
  X <- matrix(c(rep(1,length(x)),x),ncol=2)
  for (k in 1:dim(W)[2]){
    new.coef[k,] <- solve(t(X)%*%diag(W[,k])%*%X)%*%t(X)%*%diag(W[,k])%*%Y
  }
  return(t(new.coef))
}

#ii
calcNewSd <- function(W,x,Y,a,b){
  new.sd <- NULL
  #calculating new sd
  for (k in 1:dim(W)[2]){
    new.sd[k] <- sqrt(sum(W[,k]*(Y-a[k]-b[k]*x)^2)/sum(W[,k]))
  }
  return(new.sd)
}

#iii
linRegMixMstep <- function(W,x,Y){
  #calculating new coefficents for a and b
  new.coef <- calcNewCoefs(W,x,Y)
  
  #calculating new sd
  new.sd <- calcNewSd(W,x,Y,new.coef[1,],new.coef[2,]) 

  #Calculating new pi
  new.pi <- NULL
  for (k in 1:dim(W)[2]){
    new.pi[k] <- sum(W[,k])/dim(W)[1]
  }
 return(list(new.pi,new.coef[1,],new.coef[2,],new.sd))
}


#Part c
linRegMixCalcLogLik <- function(x,Y,piCur,piNew,aCur,aNew,bCur,bNew,sCur,sNew){
  #calculating conditional probabilities for current and new values of theta
  W.cur <- linRegMixEstep(piCur,aCur,bCur,sCur,x,Y)
  W.new <- linRegMixEstep(piNew,aNew,bNew,sNew,x,Y)
  #Creating object to store values of log-likelihood for each observation i of Y 
  Q.cur <- NULL
  Q.new <- NULL
  for (i in 1:length(Y)){
    #calculating log likelihood for each value of Y
    Q.cur[i] <- sum(W.cur[i,]*log(piCur))+sum(W.cur[i,]*log(dnorm(Y[i],mean = aCur+bCur*x[i],sd=sCur)))
    Q.new[i] <- sum(W.new[i,]*log(piNew))+sum(W.new[i,]*log(dnorm(Y[i],mean = aNew+bNew*x[i],sd=sNew)))
  }
  #Summing over all observations as is in the formula
  Q.cur <- sum(Q.cur)
  Q.new <- sum(Q.new)
  return(c(Q.cur,Q.new))
}


#Part d
linRegMixEM <- function(piInit,aInit,bInit,sInit,x,Y,convergeEps){
  #Initializing the algorithm
  piCur <- piInit
  aCur <- aInit
  bCur <- bInit
  sCur <- sInit
  
  conv.critera <- 0
  while(conv.critera==0){
    #E-step
    W <- linRegMixEstep(piCur,aCur,bCur,sCur,x,Y)
    #Calculating new values for parameters
    theta.new <- linRegMixMstep(W,x,Y)
    #Calculating log-likelihood for current and new values of parameters
    theta.loglik <- linRegMixCalcLogLik(x,Y,piCur,theta.new[[1]],aCur,theta.new[[2]],bCur,theta.new[[3]],sCur,theta.new[[4]])
    #checking convergence criteria
    if(abs(theta.loglik[2]-theta.loglik[1])<convergeEps){
      conv.critera=1
    }
    #Updating current values of parameters
    piCur <- theta.new[[1]]
    aCur <- theta.new[[2]]
    bCur <- theta.new[[3]]
    sCur <- theta.new[[4]]
  }
  return(theta.new)
}

#Part e
mle.est <- linRegMixEM(c(0.4,0.3,0.3),c(0.1,-0.1,0.2),c(1,-1,1),c(1,0.5,1.1),q4.data$x,q4.data$Y,1e-06)
#pi MLEs
round(mle.est[[1]],3)
# (0.450, 0.255, 0.295)

#alpha MLEs
round(mle.est[[2]],3)
# (0.838,  7.458, -5.308)

#beta MLEs
round(mle.est[[3]],3)
# (0.258, -1.755,  1.223)

#sd MLEs
round(mle.est[[4]],3)
# (0.787, 0.935, 1.450)
```

```{r}
#Q5 but with unknown number of groups 

#Need to edit as in bootstrapping dont care aout having same number in each group! (I think)

#a
bootFStat <- function(group,y,w,bootCount){
  #object to store B bootstrap estimates of f statistic
  Fstat.boot.est <- NULL
  #grouping values into dataset 
  data <- data.frame("group"=group,"Obs"=y,"weight"=w)
  #Adding id column so can sample by id to retain all info of row (obs and weight)
  data$id <- seq(1,dim(data)[1])
  
  for (b in 1:bootCount){
    #object to store value of y tidle * N 
    y.tilde.N <- 0
    
    #loop to do bootstrap method for each group
      for (group_val in unique(data$group)){
        #selecting subset id's of dataset that is current group
        group_dataset <- data$id[data$group==group_val]
        #sampling from group dataset with replacement
        group_sample.id <- sample(group_dataset, size = length(group_dataset), replace = TRUE)
        
          #Using sampled id to select obs and weight for each id sampled to create bootstrap dataset for each group
          #Selecting obs and weights using id's sampled
          sample.group.obs <- NULL
          sample.group.weight <- NULL
          
          for (i in 1:length(group_sample.id)){
            sample.group.obs[i] <- data$Obs[data$id == group_sample.id[i]]
            sample.group.weight[i] <- data$weight[data$id == group_sample.id[i]]
          }
          #Setting sampled observations and weights to be those of group they belong to before repeating loop with next group
          assign(paste0("sample.",group_val,".obs"),sample.group.obs)
          assign(paste0("sample.",group_val,".weight"),sample.group.weight)
          
          #Calculating y tilde_{i} for current group
          y.tilde.group <- sum(sample.group.obs*sample.group.weight)/length(sample.group.obs)
          assign(paste0("y.tilde.",group_val),y.tilde.group)
          
          #updating y tilde using current group
          y.tilde.N <- y.tilde.N + sum(sample.group.obs*sample.group.weight)
          
          #Calculating part of denominator of f-statistic corresponding to current group
          f.denom.group.part <- sum((sample.group.obs*sample.group.weight-y.tilde.group)^2)
          assign(paste0("f.denom.",group_val),f.denom.group.part)
      }
    
    #calculating y tilde
    y.tilde <- y.tilde.N/length(y)
    # y.tilde <- y.tilde.N/300
    
    #calculating summation parts of numerator and denominator of f statistic
    fstat.num <- 0
    fstat.denom <- 0
    for (group_val in unique(data$group)){
      fstat.num <- fstat.num+length(get(paste0("sample.",group_val,".obs")))*(get(paste0("y.tilde.",group_val))-y.tilde)^2
      fstat.denom <- fstat.denom + get(paste0("f.denom.",group_val))
    }
    #calculating F stat
    fstat <- (fstat.num/(length(unique(group))-1))/(fstat.denom/(length(y)-length(unique(group))))
    Fstat.boot.est[b] <- fstat
  }
  
  #Calculating standard error of bootstrap f statistic
  bootstr.mu <- sum(Fstat.boot.est)/length(Fstat.boot.est)
  bootstr.se <- sqrt(sum((Fstat.boot.est-bootstr.mu )^2)/(length(Fstat.boot.est-1)))
  
  return(bootstr.se)
}

#b
set.seed(2022)
bootFStat(eco.df$habitat,eco.df$density,eco.df$weight,1000)  



```

```{r}
#Bootstrap where number in each group not set

bootFStat <- function(group,y,w,bootCount){
  #object to store B bootstrap estimates of f statistic
  Fstat.boot.est <- NULL
  #grouping values into dataset 
  data <- data.frame("group"=group,"Obs"=y,"weight"=w)
  #Adding id column so can sample by id to retain all info of row (obs and weight)
  data$id <- seq(1,dim(data)[1])
  
  for (b in 1:bootCount){
    #object to store value of y tilde * N 
    y.tilde.N <- 0
    
    #sampling dataset
    sample.id <- sample(data$id, size = length(y), replace = TRUE)

    sample.obs <- NULL
    sample.weight <- NULL
    sample.group <- NULL
    #extracting weight, group and y value corresponding to id sampled
    for (i in 1:length(y)){
      sample.obs[i] <- data$Obs[data$id == sample.id[i]]
      sample.weight[i] <- data$weight[data$id == sample.id[i]]
      sample.group[i] <- data$group[data$id == sample.id[i]]
    }
    #creating dataset of sample
    sample.dataset <- data.frame("group"=sample.group,"Obs"=sample.obs,"weight"=sample.weight)
    
    #computing statistics to calculate f-stat
    for (group_val in unique(sample.dataset$group)){
      group.dataset <- data.frame("Obs"=sample.dataset$Obs[sample.dataset==group_val],
                                  "weight"=sample.dataset$weight[sample.dataset==group_val])
      
      #calculating number of obs for current group
      assign(paste0("nobs.",group_val),dim(group.dataset)[1])
      
      #Calculating y tilde_{i} for current group
      y.tilde.group <- sum(group.dataset$Obs*group.dataset$weight)/dim(group.dataset)[1]
      assign(paste0("y.tilde.",group_val),y.tilde.group)
          
      #updating y tilde using current group
      y.tilde.N <- y.tilde.N + sum(group.dataset$Obs*group.dataset$weight)
          
      #Calculating part of denominator of f-statistic corresponding to current group
      f.denom.group.part <- sum((group.dataset$Obs*group.dataset$weight-y.tilde.group)^2)
      assign(paste0("f.denom.",group_val),f.denom.group.part)
      
    }
    #calculating y tilde
    y.tilde <- y.tilde.N/length(y)
    
    #calculating summation parts of numerator and denominator of f statistic
    fstat.num <- 0
    fstat.denom <- 0
    for (group_val in unique(data$group)){
      fstat.num <- fstat.num+get(paste0("nobs.",group_val))*(get(paste0("y.tilde.",group_val))-y.tilde)^2
      fstat.denom <- fstat.denom + get(paste0("f.denom.",group_val))
    }
    #calculating F stat
    fstat <- (fstat.num/(length(unique(group))-1))/(fstat.denom/(length(y)-length(unique(group))))
    Fstat.boot.est[b] <- fstat
    
  }
  #Calculating standard error of bootstrap f statistic
  bootstr.mu <- sum(Fstat.boot.est)/length(Fstat.boot.est)
  bootstr.se <- sqrt(sum((Fstat.boot.est-bootstr.mu )^2)/(length(Fstat.boot.est-1)))
  
  return(bootstr.se)
}
#Sorry about using so many loops :P

#b
set.seed(2022)
bootFStat(eco.df$habitat,eco.df$density,eco.df$weight,1000)  
```





```{r}
## Question 5 extra stuff (use this to check results by doing piece by piece)

#a
unique(eco.df$habitat)[3]

bootFStat <- function(group,y,w,bootCount){
  #grouping into dataset 
  data <- data.frame("group"=group,"Obs"=y,"weight"=w)
  #Adding id column so can sample by id to retain all info of row (obs and weight)
  data$id <- seq(1,dim(data)[1])
  
  #Splitting data into groups depending on group
  group.A <- data$id[data$group=="A"]
  group.B <- data$id[data$group=="B"]
  group.C <- data$id[data$group=="C"]
  #Sampling with replacement
  sample.A.id <- sample(group.A, size = length(group.A), replace = TRUE)
  sample.B.id <- sample(group.B, size = length(group.B), replace = TRUE)
  sample.C.id <- sample(group.C, size = length(group.C), replace = TRUE)
  
  #Using sampled id to select obs and weight for each id sampled to create bootstrap dataset for each group
  #Selecting obs and weights using id's sampled
  
  #For A
  sample.A.obs <- NULL
  sample.A.weight <- NULL
  for (i in 1:length(sample.A.id)){
    sample.A.obs[i] <- data$Obs[data$id == sample.A.id[i]]
    sample.A.weight[i] <- data$weight[data$id == sample.A.id[i]]
  }
  #calculating y.tilde_{i} for A
  y.tilde.A <- sum(sample.A.obs*sample.A.obs)/length(sample.A.obs)
  
  #For B
  sample.B.obs <- NULL
  sample.B.weight <- NULL
  for (i in 1:length(sample.B.id)){
    sample.B.obs[i] <- data$Obs[data$id == sample.B.id[i]]
    sample.B.weight[i] <- data$weight[data$id == sample.B.id[i]]
  }
  #calculating y.tilde_{i} for B
  y.tilde.B <- sum(sample.B.obs*sample.B.obs)/length(sample.B.obs)
  
  #For C
  sample.C.obs <- NULL
  sample.C.weight <- NULL
  for (i in 1:length(sample.C.id)){
    sample.C.obs[i] <- data$Obs[data$id == sample.C.id[i]]
    sample.C.weight[i] <- data$weight[data$id == sample.C.id[i]]
  }
  #calculating y.tilde_{i} for C
  y.tilde.C <- sum(sample.C.obs*sample.C.obs)/length(sample.C.obs)
  
  #Calculating y.tilde
  y.tilde <- (y.tilde.A*length(sample.A.obs)+y.tilde.B*length(sample.B.obs)+y.tilde.C*length(sample.C.obs))/length(y)
  
  #Calculating F.tilde
  F.num <- 1
  F.denom <-2
}








library(dplyr)
eco.df <- read.table("q5EcoStudy.txt",header = T)
data <- eco.df%>%
  rename("group"="habitat","Obs"="density")
data$id <- seq(1,dim(data)[1])
  
  #Splitting data into groups depending on group
  group.A <- data$id[data$group=="A"]
  group.B <- data$id[data$group=="B"]
  group.C <- data$id[data$group=="C"]
  #Sampling with replacement
  sample.A.id <- sample(group.A, size = length(group.A), replace = TRUE)
  sample.B.id <- sample(group.B, size = length(group.B), replace = TRUE)
  sample.C.id <- sample(group.C, size = length(group.C), replace = TRUE)
  
  #Using sampled id to select obs and weight for each id sampled to create bootstrap dataset for each group
  #Selecting obs and weights using id's sampled
  
  #For A
  sample.A.obs <- NULL
  sample.A.weight <- NULL
  for (i in 1:length(sample.A.id)){
    sample.A.obs[i] <- data$Obs[data$id == sample.A.id[i]]
    sample.A.weight[i] <- data$weight[data$id == sample.A.id[i]]
  }
  #calculating y.tilde_{i} for A
  y.tilde.A <- sum(sample.A.obs*sample.A.obs)/length(sample.A.obs)


#b
eco.df <- read.table("q5EcoStudy.txt",header = T)

test
```


