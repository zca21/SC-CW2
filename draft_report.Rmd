---
title: "Statistical Computing (MATH6173) Coursework 2"
author: "Student ID: 34273638"
geometry: margin = 2cm
output:
  pdf_document: default
header-includes:
  - \usepackage{wrapfig}
  - \usepackage{lipsum}
  - \usepackage{setspace}
  - \usepackage{titlesec}
  - \usepackage{amsmath}
  - \usepackage{amssymb}
  - \usepackage{mathtools}
  - \titlespacing{\title}{0pt}{\parskip}{-\parskip}
---

```{r, include=F}
#Setting up environment
setwd("~/Desktop/Statistical Computing/CW/SC-CW2")
```

# Question 1

## Part a:

\begin{equation}
\pi(x) =
\begin{cases}
 \frac{as^{a}}{2x^{a+1}}\, ,& s<x, \\
 \frac{1}{2b}\, ,& s-b \leq x \leq s, \\
 0  ,& \text{otherwise,}
 \end{cases}  
 \end{equation}

\begin{equation}
g(x) = \frac{\lambda}{2}exp(-\lambda|x-b|)
\end{equation}

For acceptance-rejection sampling (AR sampling) we need to find a constant $M>0$ such that $\pi(x) \leq Mg(x)$ for $\pi(x)$ the target distribution and $g(x)$ the proposal density defined in equations 1 and 2 above. This constant M determines the acceptance probability which is $\frac{1}{M}$ and hence the efficiency of the algorithm. For multiple possible values of $\lambda$ in $g(x)$ the value of $\lambda$ that results in a smaller constant M will have a higher acceptance probability and thus be our preferred value of $\lambda$ for the algorithm. To find M for each $\lambda$ value I split the real domain of x into 3 regions (see plot 1) that I call 1,2 and 3 which correspond to the different expressions of $\pi$ in different regions and study the relationship between $g(x)$ and $\pi(x)$ within these 3 regions to find $M_{1},M_{2},M_{3}$ which correspond to the smallest constant such that $\pi(x) \leq M_{i}g(x)$ for the ith region. Then set $M=max(M_{1},M_{2},M_{3})$ as this is the smallest constant that satisfies $\pi(x) \leq Mg(x)$ for all possible values of x.

#### Region 1:
For the first region ($-\infty, s-b$), $\pi(x)=0$ thus we can let $M_{1}$ be some arbitrarily small value greater than 0 say $\epsilon$. This will satisfy $\pi(x) \leq M_{1}g(x)$ in the region specified as $g(x) > 0$ for all values of $x$ in this region.

#### Region 2:
For the second region [$s-b,s$], $\pi(x)=\frac{1}{2b}$. Considering that $g(x)$ is an increasing function in this region (see plot 1) it will be at it's minimum value at $x=s-b$ and that the value of $\pi(x)$ is constant. Thus, setting $M_{2}=\frac{\pi(s-b)}{g(s-b)}=\frac{exp(\lambda|s-2b|)}{\lambda b}$ will satisfy the equation $\pi(x) \leq M_{2}g(x)$ for all values of  $x$ in this region.

#### Region 3:
For the third region ($s,\infty$), $\pi(x)=\frac{as^{a}}{2x^{a+1}}$. I note that $\pi(x)$ is a decreasing function in this region (see plot 1) and initially $g(x)$ is a smaller but increasing function (see plot 1), until it reaches its maximum value at which point it is larger than $\pi(x)$. It then decreases for all larger x but always is larger than $\pi(x)$. Thus, setting $M_{3}=\frac{\pi(s)}{g(s)}=\frac{a}{\lambda s}exp(\lambda |s-b|)$ will satisfy the equation $\pi(x) \leq M_{3}g(x)$ for all $x$ in this region.

#### Calculating M and choosing lambda:
Subbing in the values of a=2, b=3 and s=1.25 given in the question the value of M for $\lambda=1$ is $M=max(\epsilon,\frac{e^{4.75}}{3},1.6e^{1.75}) \approx 38.53$ and for $\lambda=0.5$ is $M=max(\epsilon,\frac{e^{2.375}}{1.5},3.2e^{0.875})\approx 7.7$. Thus, as M is smaller for $\lambda=0.5$, 0.5 is my chosen value of lambda as it leads to an algorithm with a higher efficiency.

```{r,echo=F,fig.align='center'}
#a
#Creating plots to help explain

gx.1 <- function(x){
  gx <- 0.5*exp(-abs(x-3))
  return(gx)
}

gx.2 <- function(x){
  gx <- 0.25*exp(-0.5*abs(x-3))
  return(gx)
}

target.dist<- function(x){
  pi <-NULL
  for (i in 1:length(x)){
  if (x[i]>1.25){
    pi[i] <- (2*1.25^(2))/(2*x[i]^(3))
  } else if (x[i] >= -1.75){
    pi[i] <- 1/6
  }else{
    pi[i] <- 0
  }
  }
  return(pi)
}

x<- seq(from=-4, to=10, by=0.01)

plot(x=x,y=target.dist(x),
     type="l",
     ylab = "density",
     main="Plot 1: Comparing target and proposal pdf's",
     sub="Note: used a=2,b=3,s=1.25 with the green line seperating the regions of the graph")
curve(gx.1(x),add=T,col="red")
curve(gx.2,add=T,col="blue")
abline(v=-1.75,lty=2,col="green")
abline(v=1.25,lty=2,col="green")
text(x=-3,y=0.4,labels="Region 1")
text(x=0,y=0.4,labels="Region 2")
text(x=6,y=0.4,labels="Region 3")
legend("topright", lty = c(1,1,1), cex = 0.6, col = c("black","red","blue"),
legend = c("Target distribution pdf","g(x) pdf with lambda=1","g(x) pdf with lambda=0.5"))
```


## Part b:
To implement A-R sampling using the proposal density with $\lambda=0.5$ to obtain samples from $\pi(x)$ with parameters $s$, $a$ and $b$ I use the following algorithm:

#### Sampling from the proposal distribution:
To sample from $g(x)$ I used the inverse transformation technique by drawing $u$ from a uniform distribution then if $u<0.5$ set $x=b+\frac{log(2u)}{\lambda}$ else set $x=b-\frac{2-2u}{\lambda}$. To obtain the value $x$, drawn from $g(x)$ the proposal distribution. 

#### Accept or reject:
To either accept or reject the value $x$ I draw y from a Uniform(0,1) distribution and if: 

* $s < x$, check if $y \leq \frac{\pi(x)}{Mg(x)}=\frac{as^{a}}{2x^{a+1}}\frac{2}{M\lambda exp(-\lambda|x-b|)}$ 

* $s-b <x$, check if $y \leq \frac{\pi(x)}{Mg(x)}=\frac{1}{2b}\frac{2}{M\lambda exp(-\lambda|x-b|)}$ 

If either of these checks are true then $x$ is a draw from the target distribution $\pi(x)$ and the value is saved, if these checks are not true or $x<s-b$ then $x$ is not a draw from $\pi(x)$ and the value of $x$ is disregarded. Repeat this process until the desired number of samples from $\pi(x)$ has been attained.


## Part d:
```{r,include=F}
#c
q1.ar <- function(n,a,b,s,lambda,M){
  
    X <- NULL
    for (i in 1:n){
      repeat{
        #Drawing x from the proposal distibution 
        u <- runif(1)
        if (u<0.5){
          x <- b+log(2*u)/lambda
        }else {
          x <- b-log(2-2*u)/lambda
        }
        #Performing A-R sampling
        y <- runif(1)
        if (s < x){
          if(y <= (a*s^a)/(2*x^(a+1))/(M*lambda*exp(-lambda*abs(x-b))/2)){
            X[i]<-x
            break
          }
        } else if(s-b<x){
          if(y <= 1/(2*b)/(M*lambda*exp(-lambda*abs(x-b))/2)){
            X[i]<-x
            break
          }
        }
      }
    }
    return(X)
}

#d
set.seed(2022)
ar.sample <- q1.ar(5000,2,3,1.25,0.5,M=3.2*exp(0.875))
#Creating function to create actual pdf of target distribution
target.dist <- function(x,a,b,s){
  n<-length(x)
  y<-rep(0,n)
  for (i in 1:n){
  
  if (x[i]<s-b){
    y[i] <- 0
  }else if (x[i]>s){
    y[i] <- (a*s^a)/(2*x[i]^(a+1))
  } else {
    y[i] <- 1/(2*b)
  }
  }
  return(y)
}
```

```{r,echo=F,fig.align='center'}
#Creating curve of target distribution
hist(ar.sample,nclass=100,prob = T,
     xlab = "Values generated by AR sampling",
     main = "Simulated and actual pdf of target distribution \n with AR generated sample plotted as a histogram");
curve(target.dist(x,2,3,1.25),from=-4,to=20,n=1000,add=T, lwd= 1, lty= 1,col="red");
legend("topright", lty = c(1), cex = 0.6, col = c("red"),
legend = c("Actual pdf"))
```

```{r,include=F}
#e
#Using the plain Monte Carlo estimator of I (I_{n})

#Function to calculate the kth root of kth moment using plain Monte Carlo estimator
k.root.k.moment <- function(sample,n,k){
  output <- (1/n*sum(sample^k))^(1/k)
  return(output)
}
  
#k=2
round(k.root.k.moment(ar.sample,5000,2),3)
#2nd root of 2nd moment is 2.204 (3 d.p.)

#k=3
round(k.root.k.moment(ar.sample,5000,3),3)
#3rd root of 3rd moment is 3.350 (3 d.p.)
```

# Question 2

## Part a
For the continuous-time random walk $U=(U_{i})_{i \in \{1,...,d\}}$ conditioned on $U_{0}=u_{0}$:
\begin{equation}
U_{i} \sim \text{Normal}(\mu=U_{i-1},\sigma^{2}=t_{i}-t_{i-1}) \hspace{2em} \text{at time $t_{i}$}
\end{equation}

First by transforming $\boldsymbol{a} = (a_{1},...,a_{d})$ to the logit scale by setting $b_{i}=log(\frac{a_{i}}{1-a_{i}})$, $p_{tail}(\boldsymbol{a})=Pr(\boldsymbol{X}>\boldsymbol{a}|X_{0}=x_{0})  \equiv Pr(\boldsymbol{U}>\boldsymbol{b}|U_{0}=u_{0})=p_{tail}(\boldsymbol{b}))$, thus we can work with the sequence $U$ to find $p_{tail}(\boldsymbol{a})$. Now from equation (3):
\begin{equation*}
Pr(U_{i}>b_{i})=Pr((\sqrt{t_{i}-t_{i-1}})Z+u_{i-1}>b_{i}))=Pr\left(Z>\frac{b_{i}-u_{i-1}}{\sqrt{t_{i}-t_{i-1}}}\right) \hspace{2em}  \text{for} \hspace{1em} i \in \{0,...,d\}
\end{equation*}
For $Z$ the standard normal and $u_{i-1}$ a value drawn from $U_{i-1}$.

Therefore, to not waste any sample values let $q_{i}$ the proposal distribution for the ith random variable in $U=(U_{i})_{i \in \{1,...,d\}}$ be an exponential distribution shifted by $s_{i}$ where:

\begin{equation*}
 s_{i}=\frac{b_{i}-u_{i-1}}{\sqrt{t_{i}-t_{i-1}}} 
\end{equation*}

As the samples $\boldsymbol{w}=\{w_{1},...,w_{d}\}$ drawn from these proposal distributions $\boldsymbol{q}=\{q_{1},....,q_{d}\}$ will satisfy $w_{i}>b_{i}$, $i \in \{1,...,d\}$ all values can be used to calculate $p_{tail}(\boldsymbol{b})=p_{tail}(\boldsymbol{a})$.

To calculate $q_{i}$ however, we need to know the value $u_{i-1}$ (a value drawn from the random variable $U_{i-1}$) for each $i$ . Thus, each iteration will need to generate $\boldsymbol{u} = (u_{1},...,u_{d})$ a realization of the random walk sequence $U=(U_{i})_{i \in \{1,...,d\}}$ conditioned on $U_{0}=u_{0}$ using equation (3) with the initial value $u_{0}$ of the random walk and times $t_{i}$,  $i \in \{0,...,d\}$. Then storing these values to be accessed to generate a sample from the proposal distribution. 

## Part b

#### Transforming inputs:
The algorithm will start by transforming inputs from percentages to proportions by dividing values by 100 then applying the logit transform to $\boldsymbol{a}$ and calling the new vector of values $\boldsymbol{b}=log(\frac{\boldsymbol{a}}{1-\boldsymbol{a}})$ (as mentioned in part a)

#### Drawing values from proposal distributions:
Next for each iteration j we must simulate the random walk $U$ using equation 3 and storing these realizations $\boldsymbol{u}$ as mentioned in part a. Using these values and the relevant proposal distribution $q_{i}$ (described in part a) generate $w_{i}$ for each step $i$ the random walk sequence $U$.

#### Calculating weights:
Next calculate weights $w_{i}$ using the formula $w_{i}=\frac{\pi_{i}(w_{i})}{Q_{i}(w_{i})})$ where $\pi_{i}$ is the pdf of random variable $U_{i}$ (defined by equation 3) and $Q_{i}$ is the density of the proposal distribution $q_{i}$ an exponential distribution with rate $\lambda$ and shifted by $s_{i}$ defined in part a. 

Then the combined weight used to calculate $p_{tail}(\boldsymbol{b})$ is the product of the weights $\boldsymbol{w}=(w_{1},...,w_{d})$. This defines the weight of $\boldsymbol{U}>\boldsymbol{b}$ using the fact that $w_{i}$ is weight of $Pr(U_{i}>b_{i}|U_{i-1}>b_{i-1})$, the markov property and product of conditional probability.

#### Calculating probability:
To calculate $p_{tail}(\boldsymbol{b})$, The previous 2 steps are performed n times thus we have n values of the combined weight (which I will denote $\hat{w}$) which we then use to calculate the importance sampling estimator defined as:

\begin{equation*}
  I_{n}^{IS} = \frac{1}{n}\sum_{i=1}^{n}\hat{w}(U)\psi(U) = \frac{1}{n}\sum_{i=1}^{n}\hat{w}(U) \hspace{1em} \text{as} \hspace{1em} \psi(U)=1
\end{equation*}

Thus the IS estimator of $p_{tail}(\boldsymbol{b})$ is the mean of the n combined weights and this as discussed in part a is the IS estimator of $p_{tail}(\boldsymbol{a})$ too.


# Question 3

## Part a

#### Initialisation:
At initialization set $\boldsymbol{P}_{\boldsymbol{T}}^{cur}$ to be the initial value of $\boldsymbol{P}_{\boldsymbol{T}}$ specified in the function.

#### Drawing new proposal value:
For each iteration j start by drawing $\boldsymbol{P}_{\boldsymbol{T}}^{new}$ the proposed value of $\boldsymbol{P}_{\boldsymbol{T}}$ from the proposal density by drawing each $p_{x}'$ with $x \in \{s,c,r\}$ from Dirichlet $(20p_{x}^{(cur)})$ and combining these to get $\boldsymbol{P}_{\boldsymbol{T}}^{new}=(p_{s}',p_{c}',p_{r}')^{T}$.

#### Calculate ratio for posterior density (target distribution):
Calculate the ratio of the posterior density using the new and current values of $\boldsymbol{P}_{\boldsymbol{T}}$. First by considering that:

\begin{equation*}
 p(\boldsymbol{P}_{\boldsymbol{T}}|X,\pi) \propto p(X|\boldsymbol{P}_{\boldsymbol{T}},\pi)
\end{equation*}

We get $\frac{p(\boldsymbol{P}_{\boldsymbol{T}}^{new}|\boldsymbol{X},\pi)}{p(\boldsymbol{P}_{\boldsymbol{T}}^{(cur)}|\boldsymbol{X},\pi)}=\frac{p(\boldsymbol{X}|\boldsymbol{P}_{\boldsymbol{T}}^{new},\pi)}{p(\boldsymbol{X}|\boldsymbol{P}_{\boldsymbol{T}}^{cur},\pi)}$, thus we can calculate this ratio of densities as we can calculate the RHS.

To calculate the RHS using the Markov property and equation 8 and 9 in the coursework question sheet:

\begin{equation*}
\begin{aligned}
Pr(\boldsymbol{X}=\boldsymbol{x}|\pi,\boldsymbol{P}_{\boldsymbol{T}}) = Pr(X_{1}=x_{1}|\pi,,\boldsymbol{P}_{\boldsymbol{T}})Pr(X_{2}=x_{2}|X_{1}=x_{1},,\boldsymbol{P}_{\boldsymbol{T}})\cdot \cdot \cdot Pr(X_{n}=x_{n}|X_{n-1}=x_{n-1},,\boldsymbol{P}_{\boldsymbol{T}}) = \\ \pi_{x_{1}}\prod_{i=2}^{n}p_{T}(X_{i}=x_{i}|X_{i-1}=x_{i-1})
\end{aligned}
\end{equation*}

#### Calculate ratio for proposal density:
To calculate the ratio of the proposal density $q$ of the current and new values of $\boldsymbol{P}_{\boldsymbol{T}}$. Calculate the probability density of q=Dirichlet($20 p^{cur}_{x}$) with $v= p^{cur}_{x}$ and $v= p^{new}_{x}$ for each of the rows of $\boldsymbol{P}_{\boldsymbol{T}}^{cur}$ and $\boldsymbol{P}_{\boldsymbol{T}}^{new}$. Then calculate the the probability density of $\boldsymbol{P}_{\boldsymbol{T}}^{cur}$ and $\boldsymbol{P}_{\boldsymbol{T}}^{new}$ by finding the joint probability density of the rows $p_{x}$ by multiplying the probability densities of each row together (as independent can do this). Thus the ratio of the proposal density is:

\begin{equation*}
 \frac{q(\boldsymbol{P}_{\boldsymbol{T}}^{cur})}{q(\boldsymbol{P}_{\boldsymbol{T}}^{new})} = \frac{\prod_{x=1}^{3}q(p_{x}^{cur})}{\prod_{x=1}^{3}q(p_{x}^{new})}  \hspace{1em} \text{with} \hspace{1em} x \in \{1=s,2=c,3=r\}
\end{equation*}

#### Accept or reject proposal:
Calculate $\alpha = min \left( 1,\frac{p(\boldsymbol{P}_{\boldsymbol{T}}^{new}|X,\pi)}{p(\boldsymbol{P}_{\boldsymbol{T}}^{cur}|X,\pi)}\frac{q(\boldsymbol{P}_{\boldsymbol{T}}^{cur})}{q(\boldsymbol{P}_{\boldsymbol{T}}^{new})}\right)$ using the ratios calculated above and generate $u$ a value drawn from a Uniform(0,1) distribution and if $u<=\alpha$ set $\boldsymbol{P}_{\boldsymbol{T}}^{new}$ to be the jth value of $\boldsymbol{P}_{\boldsymbol{T}}$ and update $\boldsymbol{P}_{\boldsymbol{T}}^{cur}$ to be the value of $\boldsymbol{P}_{\boldsymbol{T}}^{new}$. Otherwise, set $\boldsymbol{P}_{\boldsymbol{T}}^{cur}$ to be jth value of $\boldsymbol{P}_{\boldsymbol{T}}$ and don't update the value of $\boldsymbol{P}_{\boldsymbol{T}}^{cur}$.

#### Iterate:
Repeat until the desired number of $\boldsymbol{P}_{\boldsymbol{T}}$ samples (specified number n plus b the specified burn in number), throw away the first b values of $\boldsymbol{P}_{\boldsymbol{T}}$. The remaining n values of $\boldsymbol{P}_{\boldsymbol{T}}$ is the estimated posterior distribution of $\boldsymbol{P}_{\boldsymbol{T}}$.

