#Setting up enviroment
setwd("~/Desktop/Statistical Computing/CW/SC-CW2")

######################### Question 1 ###############################

### Part a ###

#Creating plot to help explain choosing value of lambda with highest acceptance rate

#Note assume a=2, b=3 and s=1.25 for all functions creating pdfs below

#Function of pdf of proposal density with lambda=1
gx.1 <- function(x){
  gx <- 0.5*exp(-abs(x-3))
  return(gx)
}

#Function of pdf of proposal density with lambda=0.5
gx.2 <- function(x){
  gx <- 0.25*exp(-0.5*abs(x-3))
  return(gx)
}

#Creating function to create actual pdf of target distribution 
target.dist<- function(x){
  pi <-NULL
  for (i in 1:length(x)){
    if (x[i]>1.25){
      pi[i] <- (2*1.25^(2))/(2*x[i]^(3))
    } else if (x[i] >= -1.75){
      pi[i] <- 1/6
    }else{
      pi[i] <- 0
    }
  }
  return(pi)
}

#Creating plot
x<- seq(from=-4, to=10, by=0.01)

plot(x=x,y=target.dist(x),
     type="l",
     ylab = "density",
     main="Plot 1: Comparing target and proposal pdf's",
     sub="Note: used a=2, b=3, s=1.25 with the green line seperating the regions of the graph")
curve(gx.1(x),add=T,col="red")
curve(gx.2(x),add=T,col="blue")
abline(v=-1.75,lty=2,col="green")
abline(v=1.25,lty=2,col="green")
text(x=-3,y=0.4,labels="Region 1")
text(x=0,y=0.4,labels="Region 2")
text(x=6,y=0.4,labels="Region 3")
legend("topright", lty = c(1,1,1), cex = 0.6, col = c("black","red","blue"),
       legend = c("Target distribution pdf","g(x) pdf with lambda=1","g(x) pdf with lambda=0.5"))

### Part c ###

q1.ar <- function(n,a,b,s,lambda,M){
  #Creating vector to store values from target distribution
  X <- NULL
  for (i in 1:n){
    repeat{
      #Drawing samples x from the propsal distibution using inverse transformation technique
      u <- runif(1)
      if (u<0.5){
        x <- b+log(2*u)/lambda
      }else {
        x <- b-log(2-2*u)/lambda
      }
      #Performing Accept - Reject
      y <- runif(1)
      #Checking if in region 3
      if (s < x){
        if(y <= (a*s^a)/(2*x^(a+1))/(M*lambda*exp(-lambda*abs(x-b))/2)){
          X[i]<-x
          break
        }
        #Checking if in region 2
      } else if(s-b<x){
        if(y <= 1/(2*b)/(M*lambda*exp(-lambda*abs(x-b))/2)){
          X[i]<-x
          break
        }
      }
    }
  }
  return(X)
}

### Part d ###

set.seed(2022)
#Performing AR sampling using function created in part c and with specified values of parameters
ar.sample <- q1.ar(5000,2,3,1.25,0.5,M=3.2*exp(0.875))

#Using function created in part a to plot actual pdf of target distribution

#Creating plot of sampled distribution from AR sampling and actual curve of real pdf of target distribution 
hist(ar.sample,nclass=100,prob = T,
     xlab = "Values generated by AR sampling",
     main = "Simulated and actual pdf of target distribution");
curve(target.dist(x),from=-4,to=20,n=1000,add=T, lwd= 1, lty= 1,col="red");
legend("topright", lty = c(1), cex = 0.6, col = c("red"),
       legend = c("Actual pdf"))

### Part e ###

#Using the plain Monte Carlo estimator of I (I_{n})

#Function to calculate the kth root of kth moment using plain Monte Carlo estimator
k.root.k.moment <- function(sample,n,k){
  output <- (1/n*sum(sample^k))^(1/k)
  return(output)
}

#using k=2
round(k.root.k.moment(ar.sample,5000,2),3)
#2nd root of 2nd moment is 2.204 (3 d.p.)

#using k=3
round(k.root.k.moment(ar.sample,5000,3),3)
#3rd root of 3rd moment is 3.350 (3 d.p.)


########################################## Question 2 #################################################

### Part c ###

q2.is <- function(n,x0,time,a,lambda){
  #Turning values into proportions
  a <- a/100
  x0 <- x0/100
  #transforming a to logit scale (calling this vector b)
  b <- log((a)/(1-a))
  
  p.b.iter <- NULL
  
  if (length(time)>2){  #Case when Markov chain has more than 1 transition (d>1) thus have to simulate a random walk to calculate the proposal distribution 
    
    for (k in 1:n){
      
      #Simulating random walk
      r.w <- NULL
      r.w[1] <- x0
      for (i in 2:(length(time)-1)){
        r.w[i] <- rnorm(1,mean=r.w[i-1],sd=sqrt(time[i]-time[i-1]))
      }
      
      #generating samples from shifted exponential
      X <- NULL
      for (i in 2:length(time)){
        X[i-1] <- rexp(1,rate = lambda) + (b[i-1]-r.w[i-1])/sqrt(time[i]-time[i-1])
      }
      #Calculating weights for each observation in random walk
      v.weights <- NULL
      for (i in 2:length(time)){
        v.weights[i-1] <- dnorm(X[i-1])/dexp(X[i-1]-((b[i-1]-r.w[i-1])/sqrt(time[i]-time[i-1])),rate=lambda)
      }
      #Finding probability that X is greater than a (satisfies equation 6) for current iteration 
      p.b.iter[k] <- prod(v.weights)
    }
    #Finding IS estimator by taking mean 
    I.S.est <- mean(p.b.iter)
    
  } else { # Dealing with case that markov chain only has 1 jump so don't need to calculate random walk
    # Simplifies to case that X_{1}>a given that X_{0}=x0
    
    #generating samples from proposal shifted exponential
    X <- rexp(n,rate = lambda) + (b-x0)/sqrt(time[2]-time[1])
    #Calculating weights
    v.weights <- dnorm(X)/dexp(X-((b-x0)/sqrt(time[2]-time[1])),rate=lambda)
    #Finding I.S estimator
    I.S.est  <- mean(v.weights)
  }
  
  return(I.S.est)
}


### Part d ###
set.seed(2022)
is.estimate <- q2.is(n=100000,x0=30,time=c(0,0.7,1.2,2.5),a=c(50,70,90),lambda=0.5)
round(is.estimate,4)
# Estimate to 3 s.f. is 0.0423


########################################## Question 3 #################################################

### Part b ###

calcDirichletPDF <- function(x,a){
  #Calculating log density of x under Dirichlet distribution with parameters specified by a
  log.density <- lgamma(sum(a))-sum(lgamma(a))+sum((a-1)*log(x))
  return(log.density)
}

### Part c ###

simDirichletRV <- function(a){
  #Drawing from gamma distribution
  U <- rgamma(a,1)
  #Computing V a vector drawn from Dirichlet distribution with parameters specified by a
  V <- U/sum(U)
  
  return(V)
}


### Part d ###

q3.mcmc <- function(X,initStateProbs,initPT,burnin,n){
  #Converting X into a numeric vector to access entries of PT
  X <- ifelse(X=="s",1,(ifelse(X=="c",2,3)))
  init.state <- X[1:(length(X)-1)]
  trans.state <- X[2:length(X)]
  
  #initializing algorithm
  PT <- array(NA,dim=c(3,3,n+burnin+1))
  PT[,,1] <- initStateProbs
  proposal <- matrix(NA,ncol=3,nrow=3)
  
  #Starting loop 
  for(j in 2:(n+burnin+1)){
    
    #Generating proposal value for each row of transition matrix (each weather state p_{s},p_{c},p_{r})
    for (i in 1:length(initStateProbs)){
      proposal[i,] <- simDirichletRV(20*PT[i,,j-1])
    }
    
    #Generating posterior using equation (12) for proposal (new) and current value of PT
    posterior.cur <- posterior.new <- NULL
    for (i in 1:length(init.state)){
      posterior.cur[i] <- PT[init.state[i],trans.state[i],j-1]
      posterior.new[i] <- proposal[init.state[i],trans.state[i]]
    }
    # Calculating density of posterior using equation 12
    posterior.density.cur <- prod(posterior.cur)*initStateProbs[X[1]]
    posterior.density.new <-prod(posterior.new)*initStateProbs[X[1]]
    
    # Calculating density using proposal density for each row (p_{x})
    proposal.density.cur <- proposal.density.new <- NULL
    for (i in 1:length(initStateProbs)){
      proposal.density.new[i] <- calcDirichletPDF(proposal[i,],20*PT[i,,j-1])
      proposal.density.cur[i] <- calcDirichletPDF(PT[i,,j-1],20*PT[i,,j-1])
    }
    #How do we combine the 3 vectors to compare with PT??? - do we just multiple to get joint probability density
    proposal.density.cur <- exp(sum(proposal.density.cur))
    proposal.density.new <- exp(sum(proposal.density.new))
    
    #accept-reject
    ratio <- (posterior.density.new*proposal.density.cur)/(posterior.density.cur*proposal.density.new)
    
    if (runif(1) <= min(1,ratio) ){
      PT[,,j] <- proposal
    }else{
      PT[,,j] <- PT[,,j-1]
    }
    
  }
  #Removing burn in samples
  return(PT[,,(burnin+1):(n+burnin+1)])
}

### Part e ###

#Initial values
X <- read.table("q3Weather.txt",header = F)
X <- X$V1
initPT <- matrix(1/3,nrow=3,ncol=3)
initStateProbs <- c(1/3,1/3,1/3)
n <- 100000
burnin <- 10000

set.seed(2022)

PT.sample <- q3.mcmc(X,initStateProbs,initPT,burnin,n)

PT.expected <- matrix(c(mean(PT.sample[1,1,]),mean(PT.sample[1,2,]),mean(PT.sample[1,3,]),
                        mean(PT.sample[2,1,]),mean(PT.sample[2,2,]),mean(PT.sample[2,3,]),
                        mean(PT.sample[3,1,]),mean(PT.sample[3,2,]),mean(PT.sample[3,3,])),ncol=3,nrow=3,byrow = T)
round(PT.expected,3)

#Estimated posterior mean of P_{T}  to 3 d.p. is
# 0.313 0.346 0.341
# 0.335 0.329 0.336
# 0.329 0.351 0.319

########################################## Question 4 #################################################

### Part a ###

linRegMixEstep <- function(piCur,aCur,bCur,sCur,x,Y){
  # Initalisiong matrix to store E-Step values called prob
  prob <- matrix(data=NA,nrow = length(x),ncol = length(piCur))
  for (i in 1:length(x)){
    for (j in 1:length(piCur)){
      #Calculating probability in cell (i,j) of matrix using equation 15, 16 and 17 in the the coursework sheet
      prob[i,j] <- piCur[j]*dnorm(Y[i],mean=aCur[j]+bCur[j]*x[i],sd=sCur[j])/sum(piCur*dnorm(Y[i],mean=aCur+bCur*x[i],sd=sCur))
    }
  }
  return(prob)
}

### Part b ###

### (i)
calcNewCoefs <- function(W,x,Y){
  #Setting up matrix to store values of new coefficients in
  new.coef <- matrix(data=NA,ncol = dim(W)[2],nrow=2)
  #Creating design matrix
  X <- matrix(c(rep(1,length(x)),x),ncol=2)
  
  #Calculating a^{new) and b^{new} using equation 19 in coursework sheet
  for (k in 1:dim(W)[2]){
    new.coef[,k] <- solve(t(X)%*%diag(W[,k])%*%X)%*%t(X)%*%diag(W[,k])%*%Y
  }
  return(list("a.new"=new.coef[1,],"b.new"=new.coef[2,]))
}

### (ii)
calcNewSd <- function(W,x,Y,a,b){
  new.sd <- NULL
  #calculating new sd using equation 20 in coursework sheet
  for (k in 1:dim(W)[2]){
    new.sd[k] <- sqrt(sum(W[,k]*(Y-a[k]-b[k]*x)^2)/sum(W[,k]))
  }
  return(new.sd)
}

### (iii)

linRegMixMstep <- function(W,x,Y){
  #calculating new coefficents for a and b
  new.coef <- calcNewCoefs(W,x,Y)
  
  #calculating new sd vector
  new.sd <- calcNewSd(W,x,Y,new.coef$a.new,new.coef$b.new) 
  
  #Calculating new pi using equation 18 in coursework sheet
  new.pi <- NULL
  for (k in 1:dim(W)[2]){
    new.pi[k] <- sum(W[,k])/dim(W)[1]
  }
  return(list("pi.new"=new.pi,"a.new"=new.coef$a.new,"b.new"=new.coef$b.new,"sd.new"=new.sd))
}

### Part c ###
linRegMixCalcLogLik <- function(x,Y,piCur,piNew,aCur,aNew,bCur,bNew,sCur,sNew){
  #calculating conditional probabilities for current and new values of theta
  W.cur <- linRegMixEstep(piCur,aCur,bCur,sCur,x,Y)
  W.new <- linRegMixEstep(piNew,aNew,bNew,sNew,x,Y)
  #Creating object to store values of log-likelihood for each observation i of Y 
  Q.cur <- Q.new <-NULL

  for (i in 1:length(Y)){
    #calculating log likelihood for each observed value of Y (summing over marginal likelihoods of each value of Z_{j})
    Q.cur[i] <- sum(W.cur[i,]*log(piCur))+sum(W.cur[i,]*log(dnorm(Y[i],mean = aCur+bCur*x[i],sd=sCur)))
    Q.new[i] <- sum(W.new[i,]*log(piNew))+sum(W.new[i,]*log(dnorm(Y[i],mean = aNew+bNew*x[i],sd=sNew)))
  }
  #Summing over all observations as is in the formula in lecture slides
  Q.cur <- sum(Q.cur)
  Q.new <- sum(Q.new)
  return(c(Q.cur,Q.new))
}

### part d ###

linRegMixEM <- function(piInit,aInit,bInit,sInit,x,Y,convergeEps){
  #Initializing the algorithm
  piCur <- piInit
  aCur <- aInit
  bCur <- bInit
  sCur <- sInit
  
  conv.critera <- 0
  while(conv.critera==0){
    #E-step
    W <- linRegMixEstep(piCur,aCur,bCur,sCur,x,Y)
    #Calculating new values for parameters
    theta.new <- linRegMixMstep(W,x,Y)
    #Calculating log-likelihood for current and new values of parameters
    theta.loglik <- linRegMixCalcLogLik(x,Y,piCur,theta.new$pi.new,aCur,theta.new$a.new,bCur,theta.new$b.new,sCur,theta.new$sd.new)
    #checking convergence criteria
    if(abs(theta.loglik[2]-theta.loglik[1])<convergeEps){
      conv.critera=1
    }
    #Updating current values of parameters to those that met the convergence criteria
    piCur <- theta.new$pi.new
    aCur <- theta.new$a.new
    bCur <- theta.new$b.new
    sCur <- theta.new$sd.new
  }
  return(theta.new)
}

### Part e ###

# Importing in data
q4.data <- read.csv("q4MixLinReg.csv",header=T)
# Calculating the MLE of the parameters of the mixture of linear regressions and reporting to 3 d.p.
mle.est <- linRegMixEM(c(0.4,0.3,0.3),c(0.1,-0.1,0.2),c(1,-1,1),c(1,0.5,1.1),q4.data$x,q4.data$Y,1e-06)

#pi MLEs
round(mle.est$pi.new,3)
# (0.450, 0.255, 0.295)

#alpha MLEs
round(mle.est$a.new,3)
# (0.838,  7.458, -5.308)

#beta MLEs
round(mle.est$b.new,3)
# (0.258, -1.755,  1.223)

#sd MLEs
round(mle.est$sd.new,3)
# (0.787, 0.935, 1.450)


############################################################## Question 5 ########################################################

### Part a ###

# Function performing bootstrap
# Note: The sampling does not assume need to preserve the number sampled from each group (eg bootstrap samples can have different number 
#of observations in each group from the original sample)

bootFStat <- function(group,y,w,bootCount){
  #object to store B bootstrap estimates of f statistic
  Fstat.boot.est <- NULL
  #grouping values into dataset 
  data <- data.frame("group"=group,"Obs"=y,"weight"=w)
  #Adding id column so can sample by id to retain all info of row (obs and weight)
  data$id <- seq(1,dim(data)[1])
  
  for (b in 1:bootCount){
    #object to store value of y tilde * N 
    y.tilde.N <- 0
    
    #sampling dataset to create bootstrap dataset
    sample.id <- sample(data$id, size = length(y), replace = TRUE)
    
    #Variables to store values of ith bootstrap sample observation, weight and group
    sample.obs <- sample.weight <- sample.group <- NULL
    
    #extracting weight, group and y value corresponding to id sampled
    for (i in 1:length(y)){
      sample.obs[i] <- data$Obs[data$id == sample.id[i]]
      sample.weight[i] <- data$weight[data$id == sample.id[i]]
      sample.group[i] <- data$group[data$id == sample.id[i]]
    }
    #creating dataset of bootstrap sample
    sample.dataset <- data.frame("group"=sample.group,"Obs"=sample.obs,"weight"=sample.weight)
    
    #computing statistics to calculate f-stat (for loop to calculate statistics seperately for each group)
    for (group_val in unique(sample.dataset$group)){
      group.dataset <- data.frame("Obs"=sample.dataset$Obs[sample.dataset==group_val],
                                  "weight"=sample.dataset$weight[sample.dataset==group_val])
      
      #calculating number of obs for current group
      assign(paste0("nobs.",group_val),dim(group.dataset)[1])
      
      #Calculating y tilde_{i} for current group
      y.tilde.group <- sum(group.dataset$Obs*group.dataset$weight)/dim(group.dataset)[1]
      assign(paste0("y.tilde.",group_val),y.tilde.group)
      
      #updating y tilde using current group
      y.tilde.N <- y.tilde.N + sum(group.dataset$Obs*group.dataset$weight)
      
      #Calculating part of denominator of f-statistic corresponding to current group
      f.denom.group.part <- sum((group.dataset$Obs*group.dataset$weight-y.tilde.group)^2)
      assign(paste0("f.denom.",group_val),f.denom.group.part)
      
    }
    #calculating y tilde
    y.tilde <- y.tilde.N/length(y)
    
    #calculating summation parts of numerator and denominator of f statistic
    fstat.num <- 0
    fstat.denom <- 0
    for (group_val in unique(data$group)){
      fstat.num <- fstat.num+get(paste0("nobs.",group_val))*(get(paste0("y.tilde.",group_val))-y.tilde)^2
      fstat.denom <- fstat.denom + get(paste0("f.denom.",group_val))
    }
    #calculating F stat
    fstat <- (fstat.num/(length(unique(group))-1))/(fstat.denom/(length(y)-length(unique(group))))
    Fstat.boot.est[b] <- fstat
    
  }
  #Calculating standard error of bootstrap f statistic
  bootstr.mu <- sum(Fstat.boot.est)/length(Fstat.boot.est)
  bootstr.se <- sqrt(sum((Fstat.boot.est-bootstr.mu )^2)/(length(Fstat.boot.est-1)))
  
  return(bootstr.se)
}

### Part b ###

eco.df <- read.table("q5EcoStudy.txt",header = T)
set.seed(2022)
#calculating bootstrap estimates of standard error for dataset using 1000 bootstrap replicates
round(bootFStat(eco.df$habitat,eco.df$density,eco.df$weight,1000),3) 
# Calculated standard error of f-statistic with degrees of freedom 2 and 297 is 4.131 (3 d.p.)

